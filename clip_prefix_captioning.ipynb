{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip_prefix_captioning",
      "provenance": [],
      "authorship_tag": "ABX9TyPtrdXTy8tLZsQ4izUcXeGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/CLIP_prefix_caption/blob/main/clip_prefix_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldKaCUb_JgyC"
      },
      "outputs": [],
      "source": [
        "#@title セットアップ\n",
        "\n",
        "# install transformer & CLIP\n",
        "! pip install transformers\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# get code from github\n",
        "! git clone https://github.com/cedro3/CLIP_prefix_caption.git\n",
        "%cd CLIP_prefix_caption\n",
        "\n",
        "# install library\n",
        "! pip install cog\n",
        "! pip install redis\n",
        "\n",
        "# define function\n",
        "from predict import *\n",
        "from function import *\n",
        "\n",
        "# CLIP model + GPT2 tokenizer\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device='cuda', jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# download pretrained_model\n",
        "! mkdir pretrained_model\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=14pXWwB4Zm82rsDdvbGguLfx9F8aM7ovT', 'pretrained_model/conceptual_weights.pt', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX', 'pretrained_model/coco_weights.pt', quiet=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title モデル選択 \n",
        "select ='coco' #@param ['coco', 'conceptual']\n",
        "\n",
        "if select == 'coco':\n",
        "  model_path = 'pretrained_model/coco_weights.pt'\n",
        "else:\n",
        "  model_path = 'pretrained_model/conceptual_weights.pt'\n",
        "\n",
        "prefix_length = 10\n",
        "model = ClipCaptionModel(prefix_length)\n",
        "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
        "model = model.eval() \n",
        "model = model.to('cuda')"
      ],
      "metadata": {
        "id": "CTDtuKPNq4MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title サンプル画像表示\n",
        "folder = 'images_'+select\n",
        "display_pic(folder)"
      ],
      "metadata": {
        "id": "LXg1bvNpsXt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 画像から文の生成\n",
        "image = '05.jpg' #@param {type:\"string\"}\n",
        "image_path = 'images_'+select+'/'+image\n",
        "use_beam_search = False #@param {type:\"boolean\"}  \n",
        "\n",
        "image = io.imread(image_path)\n",
        "pil_image = PIL.Image.fromarray(image)\n",
        "#pil_img = Image(filename=UPLOADED_FILE)\n",
        "display(pil_image)\n",
        "\n",
        "image = preprocess(pil_image).unsqueeze(0).to('cuda')\n",
        "with torch.no_grad():\n",
        "    # if type(model) is ClipCaptionE2E:\n",
        "    #     prefix_embed = model.forward_image(image)\n",
        "    # else:\n",
        "    prefix = clip_model.encode_image(image).to('cuda', dtype=torch.float32)\n",
        "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "if use_beam_search:\n",
        "    generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n",
        "else:\n",
        "    generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
        "\n",
        "print('\\n')\n",
        "print(generated_text_prefix)"
      ],
      "metadata": {
        "id": "p4P15bAjVkjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}